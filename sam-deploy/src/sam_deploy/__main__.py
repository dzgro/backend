"""
SAM Deploy - Main Entry Point

Provides interactive deployment selection with 5 options:
1. Core Product Features (All Regions) - Full deployment to all 4 regions
2. AMS Features (All Regions) - AMS-only deployment to all 4 regions
3. Deploy to DEV - Points DEV alias to same version as 'live' alias
4. Deploy to Staging (Requires OTP) - Promotes Lambda aliases from DEV to STAGING
5. Deploy to Production (Requires OTP) - Promotes Lambda aliases from STAGING to PROD
"""

import sys
import time
import random
from typing import Tuple, Optional, List
from pathlib import Path

import boto3
import inquirer
from botocore.exceptions import ClientError

from sam_deploy.config.mapping import ENVIRONMENT, Region, LAMBDAS, LambdaName
from sam_deploy.builder.template_builder import TemplateBuilder
from sam_deploy.builder.build_lambdas import LambdaBuilder
from sam_deploy.builder.build_http_apis import HttpApiBuilder
from sam_deploy.builder.build_queues import QueueBuilder
from sam_deploy.builder.build_buckets import BucketBuilder
from sam_deploy.builder.build_cognito import CognitoBuilder
from sam_deploy.builder.build_assets import AssetsBuilder
from sam_deploy.executor.sam_executor import SAMExecutor
from sam_deploy.utils import docker_manager


def select_deployment_type() -> Tuple[str, Optional[List[Region]]]:
    """
    Interactive deployment type selection.

    Returns:
        Tuple of (deployment_type, regions):
        - deployment_type: 'core', 'ams', 'promote_dev', 'promote_staging', 'promote_prod'
        - regions: List of Region enum values or None for promotions
    """
    print("\n" + "=" * 80)
    print("SAM DEPLOY - Interactive Deployment Selection")
    print("=" * 80)

    # Interactive menu selection
    questions = [
        inquirer.List(
            'deployment',
            message="Select deployment type",
            choices=[
                '1. Core Product Features (All Regions)',
                '2. AMS Features (All Regions)',
                '3. Deploy to DEV (Point DEV alias to live version)',
                '4. Deploy to Staging (Requires OTP)',
                '5. Deploy to Production (Requires OTP)'
            ],
        ),
    ]

    answers = inquirer.prompt(questions)
    if not answers:
        print("[ERROR] No selection made. Exiting.")
        sys.exit(1)

    selection = answers['deployment']

    if "Core Product" in selection:
        print("\n[OK] Selected: Core Product Features")
        print("   Deploying: All APIs, Lambdas, Queues, Buckets, Cognito, Certificates")
        print("   Regions: ap-south-1")
        print("   Auto-publishes to DEV alias")
        return "core", [Region.DEFAULT]

    elif "AMS Features" in selection:
        print("\n[OK] Selected: AMS Features")
        print("   Deploying: AMS Lambdas and Queues only")
        print("   Regions: eu-west-1, us-east-1, us-west-2 (AMS-only regions)")
        return "ams", Region.ams_regions()

    elif "Deploy to DEV" in selection:
        print("\n[OK] Selected: Deploy to DEV")
        print("   Points DEV alias to the same version as 'live' alias")
        print("   Applies to: All regions")
        return "promote_dev", None

    elif "Staging" in selection:
        print("\n[OK] Selected: Deploy to Staging")
        print("   Promotes Lambda versions from DEV to STAGING alias")
        print("   Applies to: All regions")
        print("   Secure operation with email OTP verification")
        return "promote_staging", None

    elif "Production" in selection:
        print("\n[OK] Selected: Deploy to Production")
        print("   Promotes Lambda versions from STAGING to PROD alias")
        print("   Applies to: All regions")
        print("   Secure operation with email OTP verification")
        return "promote_prod", None

    else:
        print("[ERROR] Invalid selection. Exiting.")
        sys.exit(1)


def generate_random_otp() -> str:
    """Generate 6-digit random OTP"""
    return str(random.randint(100000, 999999))


def send_otp_via_ses(otp: str, recipient: str, stage: str) -> None:
    """
    Send OTP via AWS SES with stage information.

    Args:
        otp: 6-digit OTP code
        recipient: Email address to send OTP to
        stage: Target stage (staging or production)

    Raises:
        Exception: If email sending fails
    """
    ses = boto3.client('ses', region_name='ap-south-1')

    subject = f"[SECURE] SAM Deploy: Lambda Promotion OTP - {stage.upper()}"
    body = f"""
Lambda Alias Promotion OTP

Target Stage: {stage.upper()}

Your One-Time Password (OTP) for Lambda alias promotion:

{otp}

This OTP is valid for this session only.
Do not share this code with anyone.

If you did not initiate this deployment, please contact your administrator immediately.

---
Dzgro Technologies
Generated by SAM Deploy System
"""

    try:
        ses.send_email(
            Source='noreply@dzgro.com',  # Must be verified in SES
            Destination={'ToAddresses': [recipient]},
            Message={
                'Subject': {'Data': subject},
                'Body': {'Text': {'Data': body}}
            }
        )
        print(f"[EMAIL] Email sent successfully to {recipient} for {stage.upper()} promotion")
    except Exception as e:
        print(f"[ERROR] Failed to send email: {e}")
        raise


def get_all_lambda_functions() -> List[str]:
    """
    Get all Lambda function names from configuration.

    Returns:
        List of Lambda function names (without environment suffix)
    """
    lambda_names = []
    for lambda_config in LAMBDAS:
        # Use a temporary builder to get function names
        temp_builder = TemplateBuilder()
        fn_name = temp_builder.getFunctionName(lambda_config.name)
        lambda_names.append(fn_name)

    return lambda_names


def promote_lambda_aliases(
    source_alias: str,
    target_alias: str,
    stage_name: str,
    is_critical: bool = False
) -> None:
    """
    Common function to promote Lambda aliases from source to target with OTP verification.

    This function:
    1. Shows current source alias versions for all Lambdas
    2. Requests confirmation
    3. Sends OTP via SES to dzgrotechnologies@gmail.com
    4. Verifies OTP (3 attempts allowed)
    5. Gets version from source alias
    6. Updates target alias to point to the same version as source alias

    Applies to ALL 4 regions sequentially.
    Secure operation with email OTP verification.

    Args:
        source_alias: Alias to get version from ('live', 'dev', 'staging')
        target_alias: Alias to update ('dev', 'staging', 'prod')
        stage_name: Stage name for display and email ('dev', 'staging', 'production')
        is_critical: If True, displays extra warnings for critical operations
    """
    # Display header with appropriate criticality level
    if is_critical:
        print(f"\n[CRITICAL][CRITICAL] CRITICAL OPERATION: Promote to {stage_name.upper()} [CRITICAL][CRITICAL]")
    else:
        print(f"\n[DEPLOY] Promote to {stage_name.upper()}")
    print("=" * 80)

    all_lambdas = get_all_lambda_functions()
    all_regions = Region.all()

    print(f"\nThis will promote Lambda aliases across {len(all_regions)} regions:")
    for region in all_regions:
        print(f"  - {region.value}")

    # Show current state for first region (ap-south-1)
    arrow = f"{source_alias.upper()} → {target_alias.upper()}"
    print(f"\nLambdas to be promoted ({arrow}) in {all_regions[0].value}:")
    lambda_client = boto3.client('lambda', region_name=all_regions[0].value)

    for lambda_name in all_lambdas:
        try:
            # Get source alias version
            source_alias_info = lambda_client.get_alias(
                FunctionName=lambda_name,
                Name=source_alias
            )
            source_version = source_alias_info['FunctionVersion']
            print(f"  - {lambda_name}: {source_alias}=v{source_version} → {target_alias} will point to v{source_version}")
        except ClientError as e:
            if 'ResourceNotFoundException' in str(e):
                print(f"  - {lambda_name}: [WARNING] Function or '{source_alias}' alias not found")
            else:
                print(f"  - {lambda_name}: [WARNING] Could not get {source_alias} alias: {e}")

    # Confirmation
    confirm_message = f"[CRITICAL] Proceed with promotion to {stage_name.upper()} across all regions?" if is_critical else f"Proceed with promotion to {stage_name.upper()} across all regions?"
    questions = [
        inquirer.Confirm('confirm',
                        message=confirm_message,
                        default=False),
    ]
    answers = inquirer.prompt(questions)
    if not answers or not answers['confirm']:
        print("[ERROR] Promotion cancelled")
        return

    # Generate and send OTP
    otp = generate_random_otp()
    send_otp_via_ses(otp, "dzgrotechnologies@gmail.com", stage_name)
    print(f"\n[EMAIL] OTP sent to dzgrotechnologies@gmail.com")
    print("Please check your email for the OTP code.")

    # Verify OTP (3 attempts)
    for attempt in range(3):
        questions = [
            inquirer.Text('otp',
                         message=f"Enter OTP (Attempt {attempt + 1}/3)")
        ]
        answers = inquirer.prompt(questions)
        if not answers:
            print("[ERROR] OTP entry cancelled")
            return

        user_otp = answers['otp']
        if user_otp == otp:
            break
        else:
            if attempt < 2:
                print(f"[ERROR] Invalid OTP. {2 - attempt} attempts remaining.")
            else:
                print("[ERROR] Invalid OTP. Maximum attempts exceeded. Promotion aborted.")
                return

    # Update aliases to point to the same version as source alias
    print(f"\n[DEPLOY] Updating {target_alias.upper()} aliases to match '{source_alias}' alias across all regions...")
    total_success = 0
    total_attempts = 0

    for region in all_regions:
        print(f"\n[REGION] Region: {region.value}")
        lambda_client = boto3.client('lambda', region_name=region.value)

        for lambda_name in all_lambdas:
            total_attempts += 1
            try:
                # Step 1: Get the version that source alias points to
                source_alias_info = lambda_client.get_alias(
                    FunctionName=lambda_name,
                    Name=source_alias
                )
                source_version = source_alias_info['FunctionVersion']

                # Step 2: Update target alias to point to the same version as source
                lambda_client.update_alias(
                    FunctionName=lambda_name,
                    Name=target_alias,
                    FunctionVersion=source_version
                )
                print(f"  [OK] {lambda_name}: {target_alias} -> Version {source_version} (same as {source_alias})")
                total_success += 1
            except ClientError as e:
                if 'ResourceNotFoundException' in str(e):
                    print(f"  [WARNING] {lambda_name}: Not found in this region (skipped)")
                else:
                    print(f"  [ERROR] {lambda_name}: Failed - {e}")

    print(f"\n{'=' * 80}")
    print(f"[OK] Promoted {total_success}/{total_attempts} lambdas to {stage_name.upper()} successfully!")
    print(f"{'=' * 80}")


def promote_to_dev() -> None:
    """
    Promote Lambda code to DEV alias.
    Points DEV alias to the same version as 'live' alias.

    Version flow: live → dev
    Note: The 'live' alias is auto-published during SAM deployment via AutoPublishAlias.
    """
    promote_lambda_aliases(
        source_alias='live',
        target_alias='dev',
        stage_name='dev',
        is_critical=False
    )


def promote_to_staging_with_otp() -> None:
    """
    Promote Lambda versions from DEV to STAGING.
    Points STAGING alias to the same version as 'dev' alias.

    Version flow: dev → staging
    """
    promote_lambda_aliases(
        source_alias='dev',
        target_alias='staging',
        stage_name='staging',
        is_critical=False
    )


def promote_to_production_with_otp() -> None:
    """
    Promote Lambda versions from STAGING to PROD.
    Points PROD alias to the same version as 'staging' alias.

    Version flow: staging → prod
    """
    promote_lambda_aliases(
        source_alias='staging',
        target_alias='prod',
        stage_name='production',
        is_critical=True
    )


def deploy_core_product_features(regions: List[Region]) -> None:
    """
    Deploy ALL core product features to ALL 4 regions in a SINGLE STACK per region.

    Single stack contains:
    - Lambda layers (built once, reused for all regions)
    - Certificates (Auth us-east-1, API/Webhook region-specific) - 1 set for all envs
    - Cognito User Pools - 3 pools (dev, staging, prod)
    - Lambda functions - 1 function (will create 3 aliases later: dev, staging, prod)
    - HTTP APIs - 1 API with 3 stages (dev, staging, prod)
    - SQS queues - 3 queues per type (dev, staging, prod)
    - S3 buckets - 3 buckets per type (dev, staging, prod)

    Args:
        regions: List of regions to deploy to (all 4 regions)
    """
    print("\n" + "=" * 80)
    print("[MULTI-REGION] CORE PRODUCT FEATURES - Multi-Region Deployment (Single Stack)")
    print("=" * 80)
    print(f"\nDeploying to {len(regions)} regions:")
    for idx, region in enumerate(regions, 1):
        print(f"  {idx}. {region.value}")
    print(f"\nEach stack contains resources for ALL 3 environments (dev, staging, prod)")

    # Environments - all will be in the same stack
    environments = [ENVIRONMENT.DEV, ENVIRONMENT.STAGING, ENVIRONMENT.PROD]


    for region_idx, region in enumerate(regions, 1):
        print(f"\n{'=' * 80}")
        print(f"REGION {region_idx}/{len(regions)}: {region.value}")
        print(f"{'=' * 80}")

        # Create ONE builder for this region (will contain all environments)
        # Use DEV as base but builder will create resources for all 3 envs
        builder = TemplateBuilder(ENVIRONMENT.DEV)

        # Step 1: Build Lambda layers (optimized with caching)
        print(f"\n[1/11] [PACKAGE] Building Lambda layers with optimized caching...")
        from sam_deploy.builder.build_layers import LambdaLayerBuilder
        layer_builder = LambdaLayerBuilder(env=ENVIRONMENT.DEV, region=region, max_workers=4)
        current_layer_arns = layer_builder.execute_optimized()

        # Step 2: Build certificates for ALL environments together (Auth, API, Webhook, Assets)
        print(f"\n[2/11] [SECURE] Building certificates for all environments (dev, staging, prod)...")
        try:
            # Get all certificates for all environments at once
            # This creates/retrieves all 12 certificates (4 types × 3 environments) simultaneously
            all_certificates = TemplateBuilder.getWildCardCertificateArnsForAllEnvironments(environments, region)
            print(f"  [OK] All certificates ready for all environments")
        except Exception as e:
            print(f"  [ERROR] Certificate setup failed: {e}")
            raise

        # Step 3: Build Cognito User Pools (3 pools: dev, staging, prod)
        print(f"\n[3/11] [USER] Building Cognito User Pools (dev, staging, prod)...")
        try:
            from sam_deploy.builder.build_cognito import CognitoBuilder
            for env in environments:
                env_builder = TemplateBuilder(env)
                cognito_builder = CognitoBuilder(env_builder)
                # Use environment-specific auth certificate
                auth_cert_arn = all_certificates['Auth'][env.value]
                cognito_builder.execute(auth_cert_arn)
                # Copy resources to main builder
                builder.resources.update(env_builder.resources)
            print(f"  [OK] Created 3 Cognito User Pools")
        except Exception as e:
            print(f"  [ERROR] Cognito build failed: {e}")
            raise

        # Step 4: Build Lambda functions with aliases (dev, staging, prod)
        print(f"\n[4/11] [LAMBDA] Building Lambda functions with aliases (dev, staging, prod)...")
        try:
            lambda_builder = LambdaBuilder(builder)
            lambda_builder.execute_with_aliases(region, current_layer_arns, environments)
            print(f"  [OK] Lambda functions created with aliases")
        except Exception as e:
            print(f"  [ERROR] Lambda build failed: {e}")
            raise

        # Step 5: Build HTTP APIs with 3 stages (dev, staging, prod)
        print(f"\n[5/11] [API] Building HTTP APIs with 3 stages (dev, staging, prod)...")
        try:
            api_builder = HttpApiBuilder(builder)
            # Pass environment-specific certificates for each domain
            api_builder.execute_with_multi_stage(all_certificates, region, environments)
            print(f"  [OK] HTTP APIs created with 3 stages")
        except Exception as e:
            print(f"  [ERROR] HTTP API build failed: {e}")
            raise

        # Step 6: Build SQS queues (3 queues per type: dev, staging, prod)
        print(f"\n[6/11] [QUEUE] Building SQS queues for all environments...")
        try:
            for env in environments:
                env_builder = TemplateBuilder(env)
                queue_builder = QueueBuilder(env_builder)
                for lambda_config in LAMBDAS:
                    if env not in lambda_config.env:
                        continue
                    lambda_region = next((lr for lr in lambda_config.regions if lr.region == region), None)
                    if lambda_region and lambda_region.queue:
                        queue_builder.execute(lambda_config.name, lambda_region.queue, region)
                # Copy resources to main builder
                builder.resources.update(env_builder.resources)
            print(f"  [OK] SQS queues created for all 3 environments")
        except Exception as e:
            print(f"  [ERROR] Queue build failed: {e}")
            raise

        # Step 7: Build S3 buckets (3 buckets per type: dev, staging, prod)
        print(f"\n[7/11] [STORAGE] Building S3 buckets for all environments...")
        try:
            for env in environments:
                env_builder = TemplateBuilder(env)
                bucket_builder = BucketBuilder(env_builder)
                for lambda_config in LAMBDAS:
                    if env not in lambda_config.env:
                        continue
                    lambda_region = next((lr for lr in lambda_config.regions if lr.region == region), None)
                    if lambda_region and lambda_region.s3:
                        bucket_builder.execute(lambda_region.s3, region)
                # Copy resources to main builder
                builder.resources.update(env_builder.resources)
            print(f"  [OK] S3 buckets created for all 3 environments")
        except Exception as e:
            print(f"  [ERROR] Bucket build failed: {e}")
            raise

        # Step 8: Build Assets buckets and CloudFront distributions (3 environments)
        print(f"\n[8/11] [CDN] Building Assets S3 buckets and CloudFront distributions...")
        try:
            assets_builder = AssetsBuilder(builder)
            assets_builder.execute_for_all_environments(region, all_certificates, environments)
        except Exception as e:
            print(f"  [ERROR] Assets infrastructure build failed: {e}")
            raise

        # Step 9: Save template and check for changes
        print(f"\n[9/11] [SAVE] Saving SAM template and checking for changes...")
        try:
            executor = SAMExecutor(builder)
            template_path = executor.saveTemplateAsYaml(region)
            print(f"  [OK] Template saved: {template_path}")

            # Check if template has changed
            template_dict = {
                'AWSTemplateFormatVersion': '2010-09-09',
                'Transform': 'AWS::Serverless-2016-10-31',
                'Description': f'SAM {builder.env.value} template',
                'Resources': builder.resources
            }

            has_changed, current_hash = executor.has_template_changed(template_dict, 'core', region)

            if not has_changed:
                print(f"  [SKIP] No changes detected in template for {region.value}")
                print(f"  [INFO] Template hash: {current_hash[:16]}...")
                print(f"  [OK] Skipping deployment - stack is up to date")
            else:
                print(f"  [CHANGED] Template changes detected for {region.value}")
                print(f"  [INFO] New hash: {current_hash[:16]}...")

        except Exception as e:
            print(f"  [ERROR] Template save failed: {e}")
            raise

        # Step 10: Deploy (only if template changed)
        if has_changed:
            print(f"\n[10/11] [DEPLOY] Deploying to {region.value}...")
            try:
                # Deploy the changed template
                print(f"  [DEPLOY] Starting CloudFormation deployment...")
                executor.execute(region)
                print(f"  [OK] Successfully deployed to {region.value}")

                # Update template hash after successful deployment
                executor.update_template_hash(template_dict, 'core', region)

            except Exception as e:
                print(f"  [ERROR] Deployment failed: {e}")
                raise
        else:
            print(f"\n[10/11] [DEPLOY] Skipping deployment (no changes)")

        # Step 11: Create Cognito User Pool custom domains (always run - parallel creation)
        print(f"\n[11/11] [POST-DEPLOY] Creating Cognito User Pool custom domains for all environments...")
        try:
            from sam_deploy.builder.build_cognito_domain import CognitoDomainBuilder
            # Create domains for all environments in parallel with status polling
            domain_results = CognitoDomainBuilder.createUserPoolDomainsForAllEnvironments(
                environments=environments,
                auth_certificate_arns=all_certificates['Auth'],
                region=region
            )
            print(f"  [OK] Cognito custom domains created for all {len(domain_results)} environments")
        except Exception as e:
            print(f"  [WARNING] Cognito domain setup failed: {e}")
            # Don't fail deployment if domain setup has issues

    print(f"\n{'=' * 80}")
    print(f"[OK] ALL REGIONS DEPLOYED SUCCESSFULLY!")
    print(f"  - {len(regions)} stacks deployed (1 per region, each with 3 environments)")
    print(f"  - Cognito custom domains configured for all 3 environments")
    print(f"  - Assets infrastructure with CloudFront CDN for all 3 environments")
    print(f"{'=' * 80}")


def deploy_ams_features_multiregion(regions: List[Region]) -> None:
    """
    Deploy ONLY AMS features to AMS-specific regions sequentially (EU, NA, FE).

    This includes:
    - Lambda layers (built once, reused for all regions)
    - AMS Lambda functions (AmsChange, AmsPerformance)
    - AMS SQS queues (AMS_CHANGE, AMS_PERFORMANCE)

    Excludes:
    - HTTP APIs
    - S3 Buckets
    - Cognito
    - Certificates
    - Core Product Lambdas
    - ap-south-1 region (Core Product only)

    Args:
        regions: List of AMS regions to deploy to (EU, NA, FE)
    """
    print("\n" + "=" * 80)
    print("[AMS] AMS FEATURES - Multi-Region Deployment")
    print("=" * 80)
    print(f"\nDeploying to {len(regions)} regions sequentially:")
    for idx, region in enumerate(regions, 1):
        print(f"  {idx}. {region.value}")

    # Build layers ONCE (will be reused across all regions)
    layer_arns_cache = None

    for region_idx, region in enumerate(regions, 1):
        print(f"\n{'=' * 80}")
        print(f"REGION {region_idx}/{len(regions)}: {region.value}")
        print(f"{'=' * 80}")

        builder = TemplateBuilder()

        # Step 1: Build ONLY required Lambda layers for AMS (only once, reused across regions)
        if layer_arns_cache is None:
            print(f"\n[1/5] [PACKAGE] Building AMS Lambda layers (PYMONGO only)...")
            from sam_deploy.builder.build_layers import LambdaLayerBuilder
            from sam_deploy.config.mapping import LAYER_DEPENDENCIES, LAYER_NAME

            # Collect unique layers needed by AMS lambdas
            ams_layer_names = set()
            for lambda_config in LAMBDAS:
                if lambda_config.name in [LambdaName.AmsChange, LambdaName.AmsPerformance]:
                    ams_layer_names.update(lambda_config.layers)

            # Filter LAYER_DEPENDENCIES to only include AMS layers
            ams_layer_dependencies = {
                layer_name: deps
                for layer_name, deps in LAYER_DEPENDENCIES.items()
                if layer_name in ams_layer_names
            }

            print(f"   Building {len(ams_layer_dependencies)} layers: {', '.join([ln.value for ln in ams_layer_dependencies.keys()])}")

            layer_builder = LambdaLayerBuilder(env=builder.env, region=region, max_workers=4)
            layer_arns_cache = layer_builder.build_layers_parallel(ams_layer_dependencies)
        else:
            print(f"\n[1/5] [PACKAGE] Reusing Lambda layers from previous region")

        # Step 2: Build ONLY AMS Lambda functions
        print(f"\n[2/5] [LAMBDA] Building AMS Lambda functions (AmsChange, AmsPerformance)...")
        try:
            lambda_builder = LambdaBuilder(builder)
            # Filter to only AMS lambdas
            for lambda_config in LAMBDAS:
                if lambda_config.name in [LambdaName.AmsChange, LambdaName.AmsPerformance]:
                    # Check if this lambda is configured for this region
                    if any(lr.region == region for lr in lambda_config.regions):
                        lambda_builder.build_lambda_with_aliases(lambda_config, layer_arns_cache, region)
            print(f"  [OK] AMS Lambda functions created")
        except Exception as e:
            print(f"  [ERROR] Lambda build failed: {e}")
            raise

        # Step 3: Build ONLY AMS SQS queues
        print(f"\n[3/5] [QUEUE] Building AMS SQS queues...")
        try:
            queue_builder = QueueBuilder(builder)
            # Build only AMS queues
            for lambda_config in LAMBDAS:
                if lambda_config.name in [LambdaName.AmsChange, LambdaName.AmsPerformance]:
                    lambda_region = next((lr for lr in lambda_config.regions if lr.region == region), None)
                    if lambda_region and lambda_region.queue:
                        queue_builder.execute(lambda_config.name, lambda_region.queue, region)
            print(f"  [OK] AMS SQS queues created")
        except Exception as e:
            print(f"  [ERROR] Queue build failed: {e}")
            raise

        # Step 4: Save template and check for changes
        print(f"\n[4/5] [SAVE] Saving SAM template and checking for changes...")
        try:
            executor = SAMExecutor(builder)
            template_path = executor.saveTemplateAsYaml(region)
            print(f"  [OK] Template saved: {template_path}")

            # Check if template has changed
            template_dict = {
                'AWSTemplateFormatVersion': '2010-09-09',
                'Transform': 'AWS::Serverless-2016-10-31',
                'Description': f'SAM {builder.env.value} template',
                'Resources': builder.resources
            }

            has_changed, current_hash = executor.has_template_changed(template_dict, 'ams', region)

            if not has_changed:
                print(f"  [SKIP] No changes detected in AMS template for {region.value}")
                print(f"  [INFO] Template hash: {current_hash[:16]}...")
                print(f"  [OK] Skipping deployment - stack is up to date")
                # Skip to next region
                continue
            else:
                print(f"  [CHANGED] Template changes detected for {region.value}")
                print(f"  [INFO] New hash: {current_hash[:16]}...")

        except Exception as e:
            print(f"  [ERROR] Template save failed: {e}")
            raise

        # Step 5: Deploy
        print(f"\n[5/5] [DEPLOY] Deploying AMS template to {region.value}...")
        try:
            # Deploy the changed template
            print(f"  [DEPLOY] Starting CloudFormation deployment...")
            executor.execute(region)
            print(f"  [OK] Successfully deployed to {region.value}")

            # Update template hash after successful deployment
            executor.update_template_hash(template_dict, 'ams', region)

        except Exception as e:
            print(f"  [ERROR] Deployment failed: {e}")
            raise

    print(f"\n{'=' * 80}")
    print(f"[OK] ALL AMS REGIONS DEPLOYED SUCCESSFULLY!")
    print(f"{'=' * 80}")


def create_local_resources() -> None:
    """
    Create LOCAL environment resources (S3 buckets and SQS queues only).

    These resources are created locally and NOT deployed to AWS.
    - S3 buckets for all bucket types (LOCAL environment)
    - SQS queues for all queue types (LOCAL environment)
    - NO Lambda functions
    - NO EventSourceMapping (queues are not triggered by Lambda)
    """
    print("\n" + "=" * 80)
    print("[LOCAL] Creating LOCAL environment resources")
    print("=" * 80)

    builder = TemplateBuilder()

    # Create S3 buckets locally
    print("\n[1/2] [STORAGE] Creating LOCAL S3 buckets...")
    try:
        from dzgroshared.storage.model import S3Bucket
        import boto3

        s3 = boto3.client('s3', endpoint_url='http://localhost:4566')  # LocalStack

        for bucket in S3Bucket.all():
            bucket_name = builder.getBucketName(bucket)
            try:
                s3.create_bucket(Bucket=bucket_name)
                print(f"  [OK] Created: {bucket_name}")
            except Exception as e:
                if 'BucketAlreadyExists' in str(e) or 'BucketAlreadyOwnedByYou' in str(e):
                    print(f"  [INFO] Already exists: {bucket_name}")
                else:
                    print(f"  [ERROR] Failed to create {bucket_name}: {e}")
    except Exception as e:
        print(f"  [ERROR] S3 bucket creation failed: {e}")
        raise

    # Create SQS queues locally
    print("\n[2/2] [QUEUE] Creating LOCAL SQS queues...")
    try:
        from dzgroshared.sqs.model import QueueName
        import boto3

        sqs = boto3.client('sqs', endpoint_url='http://localhost:4566')  # LocalStack

        for queue_name_enum in QueueName.all():
            # Main queue
            queue_name = builder.getQueueName(queue_name_enum, 'Q')
            try:
                sqs.create_queue(QueueName=queue_name)
                print(f"  [OK] Created: {queue_name}")
            except Exception as e:
                if 'QueueAlreadyExists' in str(e):
                    print(f"  [INFO] Already exists: {queue_name}")
                else:
                    print(f"  [ERROR] Failed to create {queue_name}: {e}")

            # Dead letter queue
            dlq_name = builder.getQueueName(queue_name_enum, 'DLQ')
            try:
                sqs.create_queue(QueueName=dlq_name)
                print(f"  [OK] Created: {dlq_name}")
            except Exception as e:
                if 'QueueAlreadyExists' in str(e):
                    print(f"  [INFO] Already exists: {dlq_name}")
                else:
                    print(f"  [ERROR] Failed to create {dlq_name}: {e}")
    except Exception as e:
        print(f"  [ERROR] SQS queue creation failed: {e}")
        raise

    print(f"\n{'=' * 80}")
    print(f"[OK] LOCAL resources created successfully!")
    print(f"{'=' * 80}")


def main():
    """
    Main entry point for SAM Deploy.

    Flow:
    1. Detect environment (LOCAL vs DEV/STAGING/PROD)
    2. If LOCAL: Start Docker, create S3 buckets and SQS queues locally, stop Docker
    3. If not LOCAL: Interactive deployment selection
       - Core Product Features (All Regions) - Requires Docker
       - AMS Features (All Regions) - Requires Docker
       - Deploy to DEV (Point DEV alias to live version) - No Docker needed
       - Deploy to Staging (DEV → STAGING, OTP required) - No Docker needed
       - Deploy to Production (STAGING → PROD, OTP required) - No Docker needed
    4. Docker is only started when needed (for layer building and SAM deployments)
    """
    print("\n" + "=" * 80)
    print("SAM DEPLOY - AWS Serverless Application Model Deployment Tool")
    print("=" * 80)

    try:
        # Create a temporary builder to detect environment
        temp_builder = TemplateBuilder()
        print(f"\n[OK] Environment: {temp_builder.env.value.upper()}")

        # Interactive deployment selection
        deployment_type, regions = select_deployment_type()

        # Only start Docker for deployments that need it (core/ams)
        docker_started = False
        if deployment_type in ["core", "ams"]:
            print("\n[DOCKER] Starting Docker...")
            docker_manager.start_ubuntu_docker()
            docker_started = True

        try:
            if deployment_type == "core" and regions:
                deploy_core_product_features(regions)
            elif deployment_type == "ams" and regions:
                deploy_ams_features_multiregion(regions)
            elif deployment_type == "promote_dev":
                promote_to_dev()
            elif deployment_type == "promote_staging":
                promote_to_staging_with_otp()
            elif deployment_type == "promote_prod":
                promote_to_production_with_otp()
        finally:
            # Stop Docker if it was started
            if docker_started:
                print("\n[DOCKER] Stopping Docker...")
                try:
                    docker_manager.stop_ubuntu_docker()
                except Exception as e:
                    print(f"[WARNING] Warning: Failed to stop Docker: {e}")

        print("\n[OK] Deployment completed successfully!")

    except KeyboardInterrupt:
        print("\n\n[WARNING] Deployment interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n[ERROR] Deployment failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
